{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmoothQuant Real-INT8 Inference for PyTorch\n",
    "\n",
    "### Guangxuan Xiao\\*, Ji Lin\\*, Mickael Seznec, Julien Demouth, Song Han\n",
    "\n",
    "In this notebook, we use OPT-30B model to demonstrate the latency and memory advantages of SmoothQuant. We implement SmoothQuant real-INT8 inference for PyTorch with [CUTLASS](https://github.com/NVIDIA/cutlass) INT8 GEMM kernels, which are wrapped as PyTorch modules in [torch-int](https://github.com/Guangxuan-Xiao/torch-int).\n",
    "\n",
    "This notebook demonstrates SmoothQuant on OPT-30B because it is the largest model we can run both FP16 and INT8 inference on a single A100 GPU. For larger models requiring multiple GPUs, we recommend using the [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) implementation of SmoothQuant.\n",
    "\n",
    "In order to run this notebook, you need to install the following packages:\n",
    "\n",
    "- [smoothquant](https://github.com/mit-han-lab/smoothquant)\n",
    "- [torch-int](https://github.com/Guangxuan-Xiao/torch-int)\n",
    "- [PyTorch](https://pytorch.org/)\n",
    "- [Transformers](https://github.com/huggingface/transformers)\n",
    "- [Accelerate](https://github.com/huggingface/accelerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/smoothquant/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers.models.opt.modeling_opt import OPTForCausalLM\n",
    "from transformers import GPT2Tokenizer\n",
    "from smoothquant.opt import Int8OPTForCausalLM\n",
    "import os\n",
    "import gc\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an evaluator to see the performance of the model. We use a toy dataset (the first 1000 examples in the validation set of the Lambada dataset) to evaluate the model. You can replace it with your dataset. The conclusion should be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this demo, we have simplified the evaluation by using the first 1,000 samples from the LAMBADA dataset's validation set. We employ the \"Last Token Prediction Accuracy\" as our evaluation metric. This approximate evaluation is intended for demonstration purposes, providing simple but meaningful comparisons of relative performance between methods. For a more strict assessment, we recommend using the [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness) to obtain the \"Last Word Prediction Accuracy\" for the LAMBADA dataset, which is the reported metric in our paper.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # tokenize the dataset\n",
    "        def tokenize_function(examples):\n",
    "            example = self.tokenizer(examples['text'])\n",
    "            return example\n",
    "\n",
    "        self.dataset = self.dataset.map(tokenize_function, batched=True)\n",
    "        self.dataset.set_format(type='torch', columns=['input_ids'])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, model):\n",
    "        model.eval()\n",
    "        # The task is to predict the last word of the input.\n",
    "        total, hit = 0, 0\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        latency = 0\n",
    "        for batch in self.dataset:\n",
    "            input_ids = batch['input_ids'].cuda().unsqueeze(0)\n",
    "            label = input_ids[:, -1]\n",
    "            pad_len = 512 - input_ids.shape[1]\n",
    "            input_ids = pad(input_ids, (0, pad_len), value=1)\n",
    "            torch.cuda.synchronize()\n",
    "            start.record()\n",
    "            outputs = model(input_ids)\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            latency += start.elapsed_time(end)\n",
    "            last_token_logits = outputs.logits[:, -2-pad_len, :]\n",
    "            pred = last_token_logits.argmax(dim=-1)\n",
    "            total += label.size(0)\n",
    "            hit += (pred == label).sum().item()\n",
    "\n",
    "        acc = hit / total\n",
    "        lantecy = latency / len(self.dataset)\n",
    "        return acc, lantecy\n",
    "\n",
    "\n",
    "def print_model_size(model):\n",
    "    # https://discuss.pytorch.org/t/finding-model-size/130275\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    print('Model size: {:.3f}MB'.format(size_all_mb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('facebook/opt-125m') # opt-30b\n",
    "dataset = load_dataset('lambada', split='validation[:1000]')\n",
    "evaluator = Evaluator(dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP16 Model Accuracy and Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 238.875MB\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_fp16 \u001b[39m=\u001b[39m OPTForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      2\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mfacebook/opt-125m\u001b[39m\u001b[39m'\u001b[39m, torch_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat16, device_map\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# opt-30b\u001b[39;00m\n\u001b[1;32m      3\u001b[0m print_model_size(model_fp16)\n\u001b[0;32m----> 4\u001b[0m acc_fp16, lantecy_fp16 \u001b[39m=\u001b[39m evaluator\u001b[39m.\u001b[39;49mevaluate(model_fp16)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFP16 accuracy: \u001b[39m\u001b[39m{\u001b[39;00macc_fp16\u001b[39m}\u001b[39;00m\u001b[39m, per-sample lantecy: \u001b[39m\u001b[39m{\u001b[39;00mlantecy_fp16\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39mms\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/smoothquant/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     21\u001b[0m latency \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset:\n\u001b[0;32m---> 23\u001b[0m     input_ids \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mcuda()\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     24\u001b[0m     label \u001b[39m=\u001b[39m input_ids[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     25\u001b[0m     pad_len \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m \u001b[39m-\u001b[39m input_ids\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/smoothquant/lib/python3.8/site-packages/torch/cuda/__init__.py:217\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[39m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[39m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    218\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    221\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "model_fp16 = OPTForCausalLM.from_pretrained(\n",
    "    'facebook/opt-125m', torch_dtype=torch.float16, device_map='auto') # opt-30b\n",
    "print_model_size(model_fp16)\n",
    "acc_fp16, lantecy_fp16 = evaluator.evaluate(model_fp16)\n",
    "print(f'FP16 accuracy: {acc_fp16}, per-sample lantecy: {lantecy_fp16:.3f}ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_fp16\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmoothQuant W8A8 Quantized Model Accuracy and Latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the already smoothed and quantized OPT model at `https://huggingface.co/mit-han-lab/opt-[MODEL-SIZE]-smoothquant`, where `[MODEL-SIZE]` can be `125m`, `1.3B`, `2.7B`, `6.7B`, `13B`, `30b`, and `66b`. You can load the INT8 model with the following code:\n",
    "\n",
    "```python\n",
    "from smoothquant.opt import Int8OPTForCausalLM\n",
    "model = Int8OPTForCausalLM.from_pretrained(\"mit-han-lab/opt-30b-smoothquant\")\n",
    "```\n",
    "\n",
    "We implement the following quantization flow for OPT models, which you can see details in [smoothquant/opt.py](../smoothquant/opt.py).\n",
    "\n",
    "![quantization flow](../figures/quantization_flow.png)\n",
    "\n",
    "You can also check [generate_act_scales.py](../examples/generate_act_scales.py) and [export_int8_model.py](../examples/export_int8_model.py) to see how we smooth, quantize and export INT8 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module: model.decoder.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.0.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.0.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.0.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.0.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.0.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.0.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.0.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.0.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.1.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.1.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.1.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.1.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.1.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.1.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.1.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.1.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.2.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.2.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.2.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.2.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.2.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.2.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.2.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.2.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.3.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.3.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.3.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.3.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.3.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.3.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.3.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.3.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.4.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.4.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.4.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.4.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.4.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.4.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.4.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.4.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.5.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.5.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.5.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.5.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.5.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.5.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.5.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.5.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.6.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.6.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.6.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.6.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.6.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.6.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.6.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.6.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.7.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.7.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.7.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.7.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.7.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.7.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.7.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.7.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.8.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.8.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.8.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.8.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.8.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.8.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.8.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.8.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.9.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.9.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.9.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.9.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.9.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.9.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.9.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.9.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.10.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.10.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.10.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.10.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.10.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.10.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.10.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.10.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.11.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.11.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.11.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.11.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.11.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.11.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.11.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.11.final_layer_norm | Bias Shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "### 会报错\n",
    "# mit_model = OPTForCausalLM.from_pretrained(\n",
    "#     'mit-han-lab/opt-125m-smoothquant', device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "### debugging\n",
    "for name, module in model.named_modules():\n",
    "    if hasattr(module, 'weight') and module.weight is not None:\n",
    "        # print(f\"Module: {name} | Weight Shape: {module.weight.shape}\")\n",
    "        if hasattr(module, 'bias') and module.bias is not None:\n",
    "            print(f\"Module: {name} | Bias Shape: {module.bias.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 157.919MB\n",
      "SmoothQuant INT8 accuracy: 0.605, per-sample lantecy: 8.083ms\n"
     ]
    }
   ],
   "source": [
    "model_smoothquant = Int8OPTForCausalLM.from_pretrained(\n",
    "    'mit-han-lab/opt-125m-smoothquant', torch_dtype=torch.float16, device_map='cuda:0') # opt-30b-smoothquant\n",
    "\n",
    "# ### debug\n",
    "# for name, module in model_smoothquant.named_modules():\n",
    "#     if hasattr(module, 'weight') and module.weight is not None:\n",
    "#         # print(f\"Module: {name} | Weight Shape: {module.weight.shape}\")\n",
    "#         if hasattr(module, 'bias') and module.bias is not None:\n",
    "#             print(f\"Module: {name} | Bias Shape: {module.bias.shape}\")\n",
    "\n",
    "print_model_size(model_smoothquant)\n",
    "acc_smoothquant, lantecy_smoothquant = evaluator.evaluate(model_smoothquant)\n",
    "print(\n",
    "    f'SmoothQuant INT8 accuracy: {acc_smoothquant}, per-sample lantecy: {lantecy_smoothquant:.3f}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conlusion\n",
    "\n",
    "We can see that the SmoothQuant model has a similar accuracy as the FP16 model, but it is faster and uses less memory. This is because SmoothQuant reduces the quantization difficulty of activations and enables the use of INT8 GEMM kernels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b18562e22caa2a2bb5e6615862f7e7ce92f781ef7fc2a883871422ecfcd6595c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
