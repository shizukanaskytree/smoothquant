{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmoothQuant Real-INT8 Inference for PyTorch\n",
    "\n",
    "### Guangxuan Xiao\\*, Ji Lin\\*, Mickael Seznec, Julien Demouth, Song Han\n",
    "\n",
    "In this notebook, we use OPT-30B model to demonstrate the latency and memory advantages of SmoothQuant. We implement SmoothQuant real-INT8 inference for PyTorch with [CUTLASS](https://github.com/NVIDIA/cutlass) INT8 GEMM kernels, which are wrapped as PyTorch modules in [torch-int](https://github.com/Guangxuan-Xiao/torch-int).\n",
    "\n",
    "This notebook demonstrates SmoothQuant on OPT-30B because it is the largest model we can run both FP16 and INT8 inference on a single A100 GPU. For larger models requiring multiple GPUs, we recommend using the [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) implementation of SmoothQuant.\n",
    "\n",
    "In order to run this notebook, you need to install the following packages:\n",
    "\n",
    "- [smoothquant](https://github.com/mit-han-lab/smoothquant)\n",
    "- [torch-int](https://github.com/Guangxuan-Xiao/torch-int)\n",
    "- [PyTorch](https://pytorch.org/)\n",
    "- [Transformers](https://github.com/huggingface/transformers)\n",
    "- [Accelerate](https://github.com/huggingface/accelerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.models.opt.modeling_opt import OPTForCausalLM\n",
    "from transformers import GPT2Tokenizer\n",
    "from smoothquant.opt import Int8OPTForCausalLM\n",
    "import os\n",
    "import gc\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an evaluator to see the performance of the model. We use a toy dataset (the first 1000 examples in the validation set of the Lambada dataset) to evaluate the model. You can replace it with your dataset. The conclusion should be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this demo, we have simplified the evaluation by using the first 1,000 samples from the LAMBADA dataset's validation set. We employ the \"Last Token Prediction Accuracy\" as our evaluation metric. This approximate evaluation is intended for demonstration purposes, providing simple but meaningful comparisons of relative performance between methods. For a more strict assessment, we recommend using the [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness) to obtain the \"Last Word Prediction Accuracy\" for the LAMBADA dataset, which is the reported metric in our paper.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # tokenize the dataset\n",
    "        def tokenize_function(examples):\n",
    "            example = self.tokenizer(examples['text'])\n",
    "            return example\n",
    "\n",
    "        self.dataset = self.dataset.map(tokenize_function, batched=True)\n",
    "        self.dataset.set_format(type='torch', columns=['input_ids'])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, model):\n",
    "        model.eval()\n",
    "        # The task is to predict the last word of the input.\n",
    "        total, hit = 0, 0\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        latency = 0\n",
    "        for batch in self.dataset:\n",
    "            input_ids = batch['input_ids'].cuda().unsqueeze(0)\n",
    "            label = input_ids[:, -1]\n",
    "            pad_len = 512 - input_ids.shape[1]\n",
    "            input_ids = pad(input_ids, (0, pad_len), value=1)\n",
    "            torch.cuda.synchronize()\n",
    "            start.record()\n",
    "            outputs = model(input_ids)\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            latency += start.elapsed_time(end)\n",
    "            last_token_logits = outputs.logits[:, -2-pad_len, :]\n",
    "            pred = last_token_logits.argmax(dim=-1)\n",
    "            total += label.size(0)\n",
    "            hit += (pred == label).sum().item()\n",
    "\n",
    "        acc = hit / total\n",
    "        lantecy = latency / len(self.dataset)\n",
    "        return acc, lantecy\n",
    "\n",
    "\n",
    "def print_model_size(model):\n",
    "    # https://discuss.pytorch.org/t/finding-model-size/130275\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    print('Model size: {:.3f}MB'.format(size_all_mb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('facebook/opt-125m') # opt-30b\n",
    "dataset = load_dataset('lambada', split='validation[:1000]')\n",
    "evaluator = Evaluator(dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP16 Model Accuracy and Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 238.875MB\n",
      "FP16 accuracy: 0.605, per-sample lantecy: 9.436ms\n"
     ]
    }
   ],
   "source": [
    "model_fp16 = OPTForCausalLM.from_pretrained(\n",
    "    'facebook/opt-125m', torch_dtype=torch.float16, device_map='auto') # opt-30b\n",
    "print_model_size(model_fp16)\n",
    "acc_fp16, lantecy_fp16 = evaluator.evaluate(model_fp16)\n",
    "print(f'FP16 accuracy: {acc_fp16}, per-sample lantecy: {lantecy_fp16:.3f}ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module: model.decoder.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.0.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.0.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.0.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.0.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.0.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.0.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.0.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.0.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.1.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.1.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.1.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.1.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.1.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.1.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.1.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.1.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.2.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.2.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.2.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.2.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.2.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.2.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.2.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.2.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.3.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.3.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.3.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.3.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.3.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.3.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.3.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.3.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.4.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.4.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.4.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.4.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.4.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.4.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.4.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.4.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.5.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.5.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.5.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.5.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.5.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.5.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.5.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.5.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.6.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.6.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.6.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.6.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.6.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.6.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.6.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.6.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.7.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.7.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.7.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.7.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.7.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.7.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.7.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.7.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.8.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.8.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.8.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.8.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.8.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.8.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.8.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.8.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.9.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.9.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.9.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.9.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.9.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.9.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.9.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.9.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.10.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.10.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.10.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.10.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.10.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.10.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.10.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.10.final_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.11.self_attn.k_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.11.self_attn.v_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.11.self_attn.q_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.11.self_attn.out_proj | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.11.self_attn_layer_norm | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.11.fc1 | Bias Shape: torch.Size([3072])\n",
      "Module: model.decoder.layers.11.fc2 | Bias Shape: torch.Size([768])\n",
      "Module: model.decoder.layers.11.final_layer_norm | Bias Shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for name, module in model_fp16.named_modules():\n",
    "    if hasattr(module, 'weight') and module.weight is not None:\n",
    "        # print(f\"Module: {name} | Weight Shape: {module.weight.shape}\")\n",
    "        if hasattr(module, 'bias') and module.bias is not None:\n",
    "            print(f\"Module: {name} | Bias Shape: {module.bias.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_fp16\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmoothQuant W8A8 Quantized Model Accuracy and Latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the already smoothed and quantized OPT model at `https://huggingface.co/mit-han-lab/opt-[MODEL-SIZE]-smoothquant`, where `[MODEL-SIZE]` can be `125m`, `1.3B`, `2.7B`, `6.7B`, `13B`, `30b`, and `66b`. You can load the INT8 model with the following code:\n",
    "\n",
    "```python\n",
    "from smoothquant.opt import Int8OPTForCausalLM\n",
    "model = Int8OPTForCausalLM.from_pretrained(\"mit-han-lab/opt-30b-smoothquant\")\n",
    "```\n",
    "\n",
    "We implement the following quantization flow for OPT models, which you can see details in [smoothquant/opt.py](../smoothquant/opt.py).\n",
    "\n",
    "![quantization flow](../figures/quantization_flow.png)\n",
    "\n",
    "You can also check [generate_act_scales.py](../examples/generate_act_scales.py) and [export_int8_model.py](../examples/export_int8_model.py) to see how we smooth, quantize and export INT8 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Int8OPTForCausalLM were not initialized from the model checkpoint at int8_models/opt-125m-smoothquant.pt and are newly initialized because the shapes did not match:\n",
      "- model.decoder.layers.0.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.0.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.0.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.0.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.0.fc1.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1, 3072]) in the model instantiated\n",
      "- model.decoder.layers.0.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.1.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.1.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.1.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.1.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.1.fc1.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1, 3072]) in the model instantiated\n",
      "- model.decoder.layers.1.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.2.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.2.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.2.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.2.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.2.fc1.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1, 3072]) in the model instantiated\n",
      "- model.decoder.layers.2.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.3.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.3.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.3.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.3.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.3.fc1.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1, 3072]) in the model instantiated\n",
      "- model.decoder.layers.3.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.4.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.4.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.4.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.4.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.4.fc1.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1, 3072]) in the model instantiated\n",
      "- model.decoder.layers.4.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.5.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.5.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.5.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.5.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.5.fc1.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1, 3072]) in the model instantiated\n",
      "- model.decoder.layers.5.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.6.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.6.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.6.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.6.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.6.fc1.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1, 3072]) in the model instantiated\n",
      "- model.decoder.layers.6.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.7.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.7.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.7.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.7.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.7.fc1.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1, 3072]) in the model instantiated\n",
      "- model.decoder.layers.7.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.8.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.8.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.8.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.8.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.8.fc1.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1, 3072]) in the model instantiated\n",
      "- model.decoder.layers.8.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.9.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.9.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.9.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.9.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.9.fc1.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1, 3072]) in the model instantiated\n",
      "- model.decoder.layers.9.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.10.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.10.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.10.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.10.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.10.fc1.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1, 3072]) in the model instantiated\n",
      "- model.decoder.layers.10.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.11.self_attn.k_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.11.self_attn.v_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.11.self_attn.q_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.11.self_attn.out_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- model.decoder.layers.11.fc1.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1, 3072]) in the model instantiated\n",
      "- model.decoder.layers.11.fc2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 157.919MB\n",
      "SmoothQuant INT8 accuracy: 0.378, per-sample lantecy: 7.928ms\n"
     ]
    }
   ],
   "source": [
    "# int8_models/opt-125m-smoothquant.pt\n",
    "\n",
    "model_smoothquant = Int8OPTForCausalLM.from_pretrained(\n",
    "    'int8_models/opt-125m-smoothquant.pt', torch_dtype=torch.float16, device_map='cuda:0', ignore_mismatched_sizes=True) # opt-30b-smoothquant\n",
    "print_model_size(model_smoothquant)\n",
    "\n",
    "acc_smoothquant, lantecy_smoothquant = evaluator.evaluate(model_smoothquant)\n",
    "print(\n",
    "    f'SmoothQuant INT8 accuracy: {acc_smoothquant}, per-sample lantecy: {lantecy_smoothquant:.3f}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conlusion\n",
    "\n",
    "We can see that the SmoothQuant model has a similar accuracy as the FP16 model, but it is faster and uses less memory. This is because SmoothQuant reduces the quantization difficulty of activations and enables the use of INT8 GEMM kernels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b18562e22caa2a2bb5e6615862f7e7ce92f781ef7fc2a883871422ecfcd6595c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
